{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T9GpLuAvIwtT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirahg/Translation-Model/blob/main/Translation_Model_Adagrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DS4420 Final Project: English-French Translation Model**\n",
        "## Mirah Gordon and Jeremy Cui\n",
        "### Adagrad Optimizer"
      ],
      "metadata": {
        "id": "O6NMpMnBV5S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XMk-V8nT9IpU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLRpsedwqLg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb121e0c-1d7c-4071-8be4-447d0b9e4e68"
      },
      "source": [
        "# Create a directory and clone the Github MT-Preparation repository\n",
        "!mkdir -p nmt\n",
        "%cd nmt\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt\n",
            "fatal: destination path 'MT-Preparation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8d13pqsp3Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f570a4e2-4160-4d93-be2a-bed21b0c60f7"
      },
      "source": [
        "# Install the requirements\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.1.98)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G903Vcm7u08U"
      },
      "source": [
        "# Raw Data\n",
        "\n",
        "English-French Dataset:\n",
        "\n",
        "* EN-FR: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WmiX_xTqqdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688e1936-b1d3-4cbf-a429-a9d8fac4a5dc"
      },
      "source": [
        "# Download and unzip a dataset\n",
        "!wget https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
        "!unzip en-fr.txt.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-22 03:04:34--  https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10014972 (9.6M) [application/zip]\n",
            "Saving to: ‘en-fr.txt.zip.1’\n",
            "\n",
            "en-fr.txt.zip.1     100%[===================>]   9.55M  4.11MB/s    in 2.3s    \n",
            "\n",
            "2023-04-22 03:04:38 (4.11 MB/s) - ‘en-fr.txt.zip.1’ saved [10014972/10014972]\n",
            "\n",
            "Archive:  en-fr.txt.zip\n",
            "replace UN.en-fr.en? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace UN.en-fr.fr? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace README? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Filtering\n",
        "\n",
        "Filtering out low-quality segments can help improve the translation quality of the output MT model. This might include misalignments, empty segments, duplicates, among other issues. "
      ],
      "metadata": {
        "id": "5G6GTlXa86Qb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9jDIWarB-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835f3de7-3026-4e32-b1fa-a965516c8524"
      },
      "source": [
        "# Filter the dataset\n",
        "# Arguments: source file, target file, source language, target language\n",
        "!python3 MT-Preparation/filtering/filter.py UN.en-fr.fr UN.en-fr.en fr en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape (rows, columns): (74067, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 74067\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 60662\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 60476\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 59719\n",
            "--- HTML Removed\t\t\t--> Rows: 59719\n",
            "--- Rows will remain in true-cased\t--> Rows: 59719\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 59719\n",
            "--- Rows Shuffled\t\t\t--> Rows: 59719\n",
            "--- Source Saved: UN.en-fr.fr-filtered.fr\n",
            "--- Target Saved: UN.en-fr.en-filtered.en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization / Sub-wording\n",
        "\n",
        "MT models can only learn a specific number of vocabulary tokens due to limited hardware resources. Sub-words are used instead of whole words. At translation time, when the model sees a new word/token that looks like a word/token it has in the vocabulary, it still can try to continue the translation instead of marking this word as “unknown” or “unk”.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IbRpxXjC78c0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9c1pqhuru3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c628034-24c4-4825-da57-81568f66d634"
      },
      "source": [
        "!ls MT-Preparation/subwording/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-train_bpe.py\t1-train_unigram.py  2-subword.py  3-desubword.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weSS6QDPsOUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0f07d8-9495-4e74-b2f4-292b12805947"
      },
      "source": [
        "# Train a SentencePiece model for subword tokenization\n",
        "!python3 MT-Preparation/subwording/1-train_unigram.py UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=UN.en-fr.fr-filtered.fr --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: UN.en-fr.fr-filtered.fr\n",
            "  input_format: \n",
            "  model_prefix: source\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: UN.en-fr.fr-filtered.fr\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 59719 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=19614832\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9546% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=82\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999546\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 59719 sentences.\n",
            "unigram_model_trainer.cc(247) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(251) LOG(INFO) Extracting frequent sub strings... node_num=13632180\n",
            "unigram_model_trainer.cc(301) LOG(INFO) Initialized 74218 seed sentencepieces\n",
            "unigram_model_trainer.cc(150) [!std::isnan(score)] \n",
            "Program terminated with an unrecoverable error.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T89THXeRslKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30ac52c-3388-434b-f1a8-6a00a06e5af9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-fr.txt.zip\t README        UN.en-fr.en\t\tUN.en-fr.fr-filtered.fr\n",
            "en-fr.txt.zip.1  source.model  UN.en-fr.en-filtered.en\n",
            "MT-Preparation\t target.model  UN.en-fr.fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBWQoCfBsqlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78e582c-c358-4c89-f523-4f70dba22a7f"
      },
      "source": [
        "# Subword the dataset\n",
        "!python3 MT-Preparation/subwording/2-subword.py source.model target.model UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: UN.en-fr.fr-filtered.fr\n",
            "Target Dataset: UN.en-fr.en-filtered.en\n",
            "Done subwording the source file! Output: UN.en-fr.fr-filtered.fr.subword\n",
            "Done subwording the target file! Output: UN.en-fr.en-filtered.en.subword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnfMRckbvNfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046f4204-1a2e-41a9-baf9-74e0a5d78117"
      },
      "source": [
        "# First 3 lines before subwording\n",
        "!head -n 3 UN.en-fr.fr-filtered.fr && echo \"-----\" && head -n 3 UN.en-fr.en-filtered.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6. Invite les gouvernements, agissant en collaboration avec le mouvement coopératif, à mettre en place des programmes visant à promouvoir et renforcer la formation de ses membres, des cadres élus et, le cas échéant, des dirigeants des coopératives, ainsi qu'à créer des bases de données statistiques sur le développement des coopératives et sur leur contribution à l'économie nationale, ou à améliorer celles qui existent déjà ;\n",
            "19. Prie également le Secrétaire général de continuer à veiller, en étroite collaboration avec le Haut Commissaire et conformément au mandat qu'elle a donné à celui-ci par sa résolution 48/141 du 20 décembre 1993, à ce que, si nécessaire, des spécialistes des droits de l'homme et du droit humanitaire participent aux missions des Nations Unies pour pouvoir se pencher sur les violations graves des droits de l'homme, par exemple les exécutions extrajudiciaires, sommaires ou arbitraires ;\n",
            "1. Se félicite que le Traité sur l'AntarctiqueNations Unies, Recueil des Traités, vol. 402, no 5778. et les Traités de TlatelolcoNations Unies, Recueil des Traités, vol. 634, no 9068., de RarotongaVoir Annuaire des Nations Unies sur le désarmement, vol. 10 : 1985 (publication des Nations Unies, numéro de vente : F.86.IX.7), appendice VII., de BangkokNations Unies, Recueil des Traités, vol. 1981, no 33873. et de PelindabaA/50/426, annexe. continuent de contribuer à libérer de la présence d'armes nucléaires l'hémisphère Sud et les régions adjacentes visées par ces traités ;\n",
            "-----\n",
            "6. Invites Governments, in collaboration with the cooperative movement, to develop programmes to promote and strengthen the education of members, the elected leadership and professional cooperative management, where appropriate, and to create or improve statistical databases on the development of cooperatives and on their contribution to national economies;\n",
            "19. Also requests the Secretary-General to continue, in close collaboration with the High Commissioner, in conformity with the mandate of the High Commissioner established by the General Assembly in its resolution 48/141 of 20 December 1993, to ensure that personnel specialized in human rights and humanitarian law issues form part of United Nations missions, where appropriate, in order to deal with serious violations of human rights, such as extrajudicial, summary or arbitrary executions;\n",
            "1. Welcomes the continued contribution that the Antarctic TreatyUnited Nations, Treaty Series, vol. 402, No. 5778. and the treaties of Tlatelolco,United Nations, Treaty Series, vol. 634, No. 9068. Rarotonga,See The United Nations Disarmament Yearbook, vol. 10: 1985 (United Nations publication, Sales No. E.86.IX.7), appendix VII. BangkokUnited Nations, Treaty Series, vol. 1981, No. 33873. and PelindabaA/50/426, annex. are making towards freeing the southern hemisphere and adjacent areas covered by those treaties from nuclear weapons;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs_xxKK_vf1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086f4fa1-5042-4a29-8bfc-900ae9d7830b"
      },
      "source": [
        "# First 3 lines after subwording\n",
        "!head -n 3 UN.en-fr.fr-filtered.fr.subword && echo \"---\" && head -n 3 UN.en-fr.en-filtered.en.subword"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁ 6 . ▁Invite ▁les ▁gouvernements , ▁agissant ▁en ▁collaboration ▁avec ▁le ▁mouvement ▁coopér atif , ▁à ▁mettre ▁en ▁place ▁des ▁programmes ▁visant ▁à ▁promouvoir ▁et ▁renforcer ▁la ▁formation ▁de ▁ses ▁membres , ▁des ▁cadres ▁élus ▁et , ▁le ▁cas ▁échéant , ▁des ▁dirigeants ▁des ▁coopératives , ▁ainsi ▁qu ' à ▁créer ▁des ▁bases ▁de ▁données ▁statistiques ▁sur ▁le ▁développement ▁des ▁coopératives ▁et ▁sur ▁leur ▁contribution ▁à ▁l ' économie ▁nationale , ▁ou ▁à ▁améliorer ▁celles ▁qui ▁existent ▁déjà ▁;\n",
            "▁ 1 9 . ▁Prie ▁également ▁le ▁Secrétaire ▁général ▁de ▁continuer ▁à ▁veiller , ▁en ▁étroite ▁collaboration ▁avec ▁le ▁Haut ▁Commissaire ▁et ▁conformément ▁au ▁mandat ▁qu ' elle ▁a ▁donné ▁à ▁celui - ci ▁par ▁sa ▁résolution ▁ 4 8 / 1 4 1 ▁du ▁ 2 0 ▁décembre ▁ 1 9 9 3 , ▁à ▁ce ▁que , ▁si ▁nécessaire , ▁des ▁spécialistes ▁des ▁droits ▁de ▁l ' homme ▁et ▁du ▁droit ▁humanitaire ▁participent ▁aux ▁missions ▁des ▁Nations ▁Unies ▁pour ▁pouvoir ▁se ▁pencher ▁sur ▁les ▁violations ▁graves ▁des ▁droits ▁de ▁l ' homme , ▁par ▁exemple ▁les ▁exécutions ▁e xtrajudiciaires , ▁sommaires ▁ou ▁arbitraires ▁;\n",
            "▁ 1 . ▁Se ▁félicite ▁que ▁le ▁Traité ▁sur ▁l ' Antarctique Nations ▁Unies , ▁Recueil ▁des ▁Traités , ▁vol . ▁ 4 0 2 , ▁no ▁ 5 7 7 8 . ▁et ▁les ▁Traités ▁de ▁Tlatelolco Nations ▁Unies , ▁Recueil ▁des ▁Traités , ▁vol . ▁ 6 3 4 , ▁no ▁ 9 0 6 8 ., ▁de ▁Rarotonga Voir ▁ Annuaire ▁des ▁Nations ▁Unies ▁sur ▁le ▁désarmement , ▁vol . ▁ 1 0 ▁: ▁ 1 9 8 5 ▁( publication ▁des ▁Nations ▁Unies , ▁numéro ▁de ▁vente ▁: ▁F . 8 6 . IX . 7 ), ▁appendice ▁VII ., ▁de ▁Bangkok Nations ▁Unies , ▁Recueil ▁des ▁Traités , ▁vol . ▁ 1 9 8 1 , ▁no ▁ 3 3 8 7 3 . ▁et ▁de ▁Pelindaba A / 5 0 / 4 2 6 , ▁annexe . ▁continuent ▁de ▁contribuer ▁à ▁libérer ▁de ▁la ▁présence ▁d ' armes ▁nucléaires ▁l ' hémisphère ▁Sud ▁et ▁les ▁régions ▁adjacentes ▁visées ▁par ▁ces ▁traités ▁;\n",
            "---\n",
            "▁ 6 . ▁Invites ▁Governments , ▁in ▁collaboration ▁with ▁the ▁cooperative ▁movement , ▁to ▁develop ▁programmes ▁to ▁promote ▁and ▁strengthen ▁the ▁education ▁of ▁members , ▁the ▁elected ▁leadership ▁and ▁professional ▁cooperative ▁management , ▁where ▁appropriate , ▁and ▁to ▁create ▁or ▁improve ▁statistic al ▁databases ▁on ▁the ▁development ▁of ▁cooperatives ▁and ▁on ▁their ▁contribution ▁to ▁national ▁economies ;\n",
            "▁ 1 9 . ▁A lso ▁requests ▁the ▁Secretary - General ▁to ▁continue , ▁in ▁close ▁collaboration ▁with ▁the ▁High ▁Commissioner , ▁in ▁conformity ▁with ▁the ▁mandate ▁of ▁the ▁High ▁Commissioner ▁established ▁by ▁the ▁General ▁Assembly ▁in ▁its ▁resolution ▁ 4 8 / 1 4 1 ▁of ▁ 2 0 ▁December ▁ 1 9 9 3 , ▁to ▁ensure ▁that ▁personnel ▁specialized ▁in ▁human ▁rights ▁and ▁humanitarian ▁law ▁issues ▁form ▁part ▁of ▁Unit ed ▁Nations ▁missions , ▁where ▁appropriate , ▁in ▁order ▁to ▁deal ▁with ▁serious ▁violations ▁of ▁human ▁rights , ▁such ▁as ▁extrajudicial , ▁summary ▁or ▁arbitrar y ▁executions ;\n",
            "▁ 1 . ▁Welcomes ▁the ▁continued ▁contribution ▁that ▁the ▁Antarctic ▁Treaty Unit ed ▁Nations , ▁Treaty ▁Series , ▁vol . ▁ 4 0 2 , ▁No . ▁ 5 7 7 8 . ▁and ▁the ▁treaties ▁of ▁Tlatelolco , Unit ed ▁Nations , ▁Treaty ▁Series , ▁vol . ▁ 6 3 4 , ▁No . ▁ 9 0 6 8 . ▁Rarotonga , See ▁The ▁Unit ed ▁Nations ▁Disarmament ▁Yearbook , ▁vol . ▁ 1 0 : ▁ 1 9 8 5 ▁( Unit ed ▁Nations ▁publication , ▁Sales ▁No . ▁E . 8 6 . I X . 7 ), ▁appendix ▁VII . ▁Bangkok Unit ed ▁Nations , ▁Treaty ▁Series , ▁vol . ▁ 1 9 8 1 , ▁No . ▁ 3 3 8 7 3 . ▁and ▁Pelindaba A / 5 0 / 4 2 6 , ▁annex . ▁are ▁making ▁towards ▁free ing ▁the ▁southern ▁hemisphere ▁and ▁adj a cent ▁areas ▁covered ▁by ▁th ose ▁treaties ▁from ▁nuclear ▁weapons ;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting\n",
        "\n",
        "Split the dataset into 3 portions:\n",
        "\n",
        "1. training dataset - used for training the model;\n",
        "2. development dataset - used to run regular validations during the training to help improve the model parameters; and\n",
        "3. testing dataset - a holdout dataset used after the model finishes training to finally evaluate the model on unseen data."
      ],
      "metadata": {
        "id": "YgTZ-m718neI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfQRMGRixBAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4195cb3b-441f-4e4d-c017-9b5af12d2ac6"
      },
      "source": [
        "# Split the dataset into training set, development set, and test set\n",
        "# Development and test sets should be between 1000 and 5000 segments (here we chose 2000)\n",
        "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 UN.en-fr.fr-filtered.fr.subword UN.en-fr.en-filtered.en.subword"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (59719, 2)\n",
            "--- Empty Cells Deleted --> Rows: 59719\n",
            "--- Wrote Files\n",
            "Done!\n",
            "Output files\n",
            "UN.en-fr.fr-filtered.fr.subword.train\n",
            "UN.en-fr.en-filtered.en.subword.train\n",
            "UN.en-fr.fr-filtered.fr.subword.dev\n",
            "UN.en-fr.en-filtered.en.subword.dev\n",
            "UN.en-fr.fr-filtered.fr.subword.test\n",
            "UN.en-fr.en-filtered.en.subword.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y3HQr4nxYib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4199b6-f5d2-44f8-a350-7be3634a2a8d"
      },
      "source": [
        "# Line count for the subworded train, dev, test datatest\n",
        "!wc -l *.subword.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2000 UN.en-fr.en-filtered.en.subword.dev\n",
            "    2000 UN.en-fr.en-filtered.en.subword.test\n",
            "   55719 UN.en-fr.en-filtered.en.subword.train\n",
            "    2000 UN.en-fr.fr-filtered.fr.subword.dev\n",
            "    2000 UN.en-fr.fr-filtered.fr.subword.test\n",
            "   55719 UN.en-fr.fr-filtered.fr.subword.train\n",
            "  119438 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0duUCLP93GKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb419759-4bc3-401e-962f-271408f41a99"
      },
      "source": [
        "# Check the first and last line from each dataset\n",
        "\n",
        "# -------------------------------------------\n",
        "# Change this cell to print your name\n",
        "!echo -e \"My name is: FirstName SecondName \\n\"\n",
        "# -------------------------------------------\n",
        "\n",
        "!echo \"---First line---\"\n",
        "!head -n 1 *.{train,dev,test}\n",
        "\n",
        "!echo -e \"\\n---Last line---\"\n",
        "!tail -n 1 *.{train,dev,test}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is: FirstName SecondName \n",
            "\n",
            "---First line---\n",
            "==> UN.en-fr.en-filtered.en.subword.train <==\n",
            "▁ 6 . ▁Invites ▁Governments , ▁in ▁collaboration ▁with ▁the ▁cooperative ▁movement , ▁to ▁develop ▁programmes ▁to ▁promote ▁and ▁strengthen ▁the ▁education ▁of ▁members , ▁the ▁elected ▁leadership ▁and ▁professional ▁cooperative ▁management , ▁where ▁appropriate , ▁and ▁to ▁create ▁or ▁improve ▁statistic al ▁databases ▁on ▁the ▁development ▁of ▁cooperatives ▁and ▁on ▁their ▁contribution ▁to ▁national ▁economies ;\n",
            "\n",
            "==> UN.en-fr.fr-filtered.fr.subword.train <==\n",
            "▁ 6 . ▁Invite ▁les ▁gouvernements , ▁agissant ▁en ▁collaboration ▁avec ▁le ▁mouvement ▁coopér atif , ▁à ▁mettre ▁en ▁place ▁des ▁programmes ▁visant ▁à ▁promouvoir ▁et ▁renforcer ▁la ▁formation ▁de ▁ses ▁membres , ▁des ▁cadres ▁élus ▁et , ▁le ▁cas ▁échéant , ▁des ▁dirigeants ▁des ▁coopératives , ▁ainsi ▁qu ' à ▁créer ▁des ▁bases ▁de ▁données ▁statistiques ▁sur ▁le ▁développement ▁des ▁coopératives ▁et ▁sur ▁leur ▁contribution ▁à ▁l ' économie ▁nationale , ▁ou ▁à ▁améliorer ▁celles ▁qui ▁existent ▁déjà ▁;\n",
            "\n",
            "==> UN.en-fr.en-filtered.en.subword.dev <==\n",
            "▁ 2 8 . ▁Commends ▁the ▁Food ▁and ▁Agriculture ▁Organization ▁of ▁the ▁Unit ed ▁Nations ▁for ▁its ▁activities ▁in ▁combating ▁illegal , ▁unreported ▁and ▁unreg ulated ▁fishing , ▁including ▁its ▁initiative ▁to ▁organize ▁the ▁in tergovernmental ▁technical ▁consultation ▁on ▁illegal , ▁unreported ▁and ▁unreg ulated ▁fishing ▁and ▁fleet ▁overcapacity , ▁to ▁be ▁held ▁in ▁June ▁ 2 0 0 4 , ▁and ▁the ▁in tergovernmental ▁technical ▁consultation ▁on ▁the ▁role ▁of ▁the ▁port ▁State ▁in ▁combating ▁illegal , ▁unreported ▁and ▁unreg ulated ▁fishing , ▁to ▁be ▁held ▁in ▁September ▁ 2 0 0 4 ;\n",
            "\n",
            "==> UN.en-fr.fr-filtered.fr.subword.dev <==\n",
            "▁ 2 8 . ▁Félicite ▁l ' Organisation ▁des ▁Nations ▁Unies ▁pour ▁l ' alimentation ▁et ▁l ' agriculture ▁pour ▁l ' action ▁qu ' elle ▁mène ▁contre ▁la ▁pêche ▁illégale , ▁non ▁déclarée ▁et ▁non ▁réglementée , ▁notamment ▁pour ▁son ▁initiative ▁d ' organiser ▁la ▁C onsultation ▁technique ▁inter gouvernementale ▁sur ▁la ▁pêche ▁illégale , ▁non ▁déclarée ▁et ▁non ▁réglementée ▁et ▁la ▁surcapacité ▁des ▁flottes , ▁qui ▁aura ▁lieu ▁en ▁juin ▁ 2 0 0 4 , ▁et ▁la ▁C onsultation ▁technique ▁inter gouvernementale ▁sur ▁le ▁rôle ▁de ▁l ' État ▁du ▁port ▁dans ▁la ▁lutte ▁contre ▁la ▁pêche ▁illégale , ▁non ▁déclarée ▁et ▁non ▁réglementée , ▁prévue ▁en ▁septembre ▁ 2 0 0 4 ▁;\n",
            "\n",
            "==> UN.en-fr.en-filtered.en.subword.test <==\n",
            "▁ 1 6 . ▁Calls ▁upon ▁UN - Habitat ▁to ▁strengthen ▁efforts ▁to ▁coordinate ▁and ▁implement ▁its ▁norm ative ▁and ▁operational ▁activities ▁through ▁the ▁enhanced ▁norm ative ▁and ▁operational ▁framework ▁elaborat ed ▁in ▁the ▁medium - term ▁strategic ▁and ▁institutional ▁plan , ▁reinforcing ▁its ▁norm ative ▁activities , ▁and ▁invites ▁all ▁countries ▁in ▁a ▁position ▁to ▁do ▁so ▁to ▁support ▁the ▁activities ▁of ▁UN - Habitat ▁in ▁this ▁regard ;\n",
            "\n",
            "==> UN.en-fr.fr-filtered.fr.subword.test <==\n",
            "▁ 1 6 . ▁Demande ▁à ▁ONU - Habitat ▁de ▁redoubler ▁d ' efforts ▁en ▁vue ▁de ▁coordonner ▁et ▁de ▁mettre ▁en ▁œuvre ▁des ▁activités ▁normatives ▁et ▁opérationnelles ▁à ▁l ' aide ▁du ▁cadre ▁normatif ▁et ▁opérationnel ▁renforcé ▁exposé ▁dans ▁le ▁plan ▁stratégique ▁et ▁institutionnel ▁à ▁moyen ▁terme , ▁et ▁invite ▁tous ▁les ▁pays ▁en ▁mesure ▁de ▁le ▁faire ▁à ▁appuyer ▁les ▁activités ▁d ' ONU - Habitat ▁à ▁cet ▁égard ▁;\n",
            "\n",
            "---Last line---\n",
            "==> UN.en-fr.en-filtered.en.subword.train <==\n",
            "▁ 2 2 . ▁Acknowledges ▁that ▁the ▁outcome ▁of ▁the ▁World ▁Conference ▁against ▁Racism , ▁Raci al ▁Discrimination , ▁ X enophobia ▁and ▁Relat ed ▁Intolerance ▁is ▁on ▁an ▁equal ▁footing ▁with ▁the ▁outcomes ▁of ▁all ▁the ▁major ▁Unit ed ▁Nations ▁conferences , ▁summits ▁and ▁special ▁sessions ▁in ▁the ▁human ▁rights ▁and ▁social ▁fields ;\n",
            "\n",
            "==> UN.en-fr.fr-filtered.fr.subword.train <==\n",
            "▁ 2 2 . ▁Considère ▁que ▁les ▁décisions ▁de ▁la ▁Conférence ▁mondiale ▁contre ▁le ▁racisme , ▁la ▁discrimination ▁raciale , ▁la ▁xénophobie ▁et ▁l ' intolérance ▁qui ▁y ▁est ▁associée ▁sont ▁à ▁mettre ▁sur ▁le ▁même ▁plan ▁que ▁les ▁décisions ▁de ▁toutes ▁les ▁grandes ▁conférences , ▁réunions ▁au ▁sommet ▁et ▁sessions ▁extraordinaires ▁des ▁Nations ▁Unies ▁consacrées ▁aux ▁droits ▁de ▁l ' homme ▁et ▁aux ▁questions ▁sociales ▁;\n",
            "\n",
            "==> UN.en-fr.en-filtered.en.subword.dev <==\n",
            "▁Recalling ▁its ▁resolution ▁ 6 0 / 6 5 ▁of ▁ 8 ▁December ▁ 2 0 0 5 ,\n",
            "\n",
            "==> UN.en-fr.fr-filtered.fr.subword.dev <==\n",
            "▁Rappelant ▁sa ▁résolution ▁ 6 0 / 6 5 ▁du ▁ 8 ▁décembre ▁ 2 0 0 5 ,\n",
            "\n",
            "==> UN.en-fr.en-filtered.en.subword.test <==\n",
            "▁ 3 . ▁Consider s ▁it ▁intoler able ▁that ▁ 8 2 6 ▁million ▁people , ▁most ▁of ▁them ▁women ▁and ▁children , ▁through out ▁the ▁world ▁and ▁particularly ▁in ▁developing ▁countries , ▁do ▁not ▁have ▁en ough ▁food ▁to ▁meet ▁their ▁basi c ▁nutrition al ▁needs , ▁which ▁infringe s ▁upon ▁their ▁fundamental ▁human ▁rights ▁and ▁at ▁the ▁same ▁time ▁can ▁generate ▁additional ▁pressure s ▁on ▁the ▁environment ▁in ▁ecological ly ▁fragile ▁areas ;\n",
            "\n",
            "==> UN.en-fr.fr-filtered.fr.subword.test <==\n",
            "▁ 3 . ▁Estime ▁qu ' il ▁est ▁in tolérable ▁que ▁ 8 2 6 ▁millions ▁de ▁personnes ▁dans ▁le ▁monde , ▁pour ▁la ▁plupart ▁des ▁femmes ▁et ▁des ▁enfants , ▁en ▁particulier ▁dans ▁les ▁pays ▁en ▁développement , ▁n ' aient ▁pas ▁suffisamment ▁à ▁ man ger ▁pour ▁satisfaire ▁leurs ▁besoins ▁nutritionnels ▁essentiels , ▁ce ▁qui ▁porte ▁atteinte ▁à ▁leurs ▁droits ▁fondamentaux ▁et ▁peut , ▁parallèlement , ▁faire ▁peser ▁des ▁pr essions ▁supplémentaires ▁sur ▁l ' environnement ▁dans ▁les ▁zones ▁écologiquement ▁fragiles ▁;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9GpLuAvIwtT"
      },
      "source": [
        "# Mount your drive to save your data\n",
        "\n",
        "Click the folder icon to the left, and then click the Google Drive icon.\n",
        "\n",
        "![mount-drive.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TnAr4M7ebcz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c5f558-7422-4135-80da-e1c1dc75ef88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZjU1Lx8IxmS"
      },
      "source": [
        "# Copy your data to your Google Drive\n",
        "!cp -R /content/nmt/ /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OpenNMT-py 3.x\n",
        "!pip3 install OpenNMT-py"
      ],
      "metadata": {
        "id": "NfwkjGB3eJte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c34b3c4-1f1f-4bf4-984d-205ae7a9483b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting OpenNMT-py\n",
            "  Using cached OpenNMT_py-3.1.1-py3-none-any.whl (227 kB)\n",
            "Collecting torch<2,>=1.13\n",
            "  Using cached torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
            "Collecting sacrebleu\n",
            "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "Collecting pyahocorasick\n",
            "  Using cached pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
            "Collecting ctranslate2<4,>=3.2\n",
            "  Using cached ctranslate2-3.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.7 MB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.9/dist-packages (from OpenNMT-py) (2.2.3)\n",
            "Collecting rapidfuzz\n",
            "  Using cached rapidfuzz-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from OpenNMT-py) (6.0)\n",
            "Collecting pyonmttok<2,>=1.35\n",
            "  Using cached pyonmttok-1.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "Collecting configargparse\n",
            "  Using cached ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting waitress\n",
            "  Using cached waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.9/dist-packages (from OpenNMT-py) (2.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from ctranslate2<4,>=3.2->OpenNMT-py) (1.22.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (67.6.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.40.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.27.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.53.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<2,>=1.13->OpenNMT-py) (4.5.0)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (6.4.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (8.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (3.1.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu->OpenNMT-py) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu->OpenNMT-py) (4.9.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu->OpenNMT-py) (2022.10.31)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask->OpenNMT-py) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pyahocorasick, portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cudnn-cu11, torch, OpenNMT-py\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenNMT-py-3.1.1 colorama-0.4.6 configargparse-1.5.3 ctranslate2-3.12.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 pyahocorasick-2.0.0 pyonmttok-1.37.1 rapidfuzz-3.0.0 sacrebleu-2.3.1 torch-1.13.1 waitress-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Datasets"
      ],
      "metadata": {
        "id": "H5nXtsnReQT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the folder where you saved your prepapred datasets from the first exercise\n",
        "# You might need to mount your Google Drive first\n",
        "%cd /content/drive/MyDrive/nmt/\n",
        "!ls"
      ],
      "metadata": {
        "id": "RS9cjn_HeKIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25168d98-0e0e-4331-c99d-09e61426c981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nmt\n",
            "Anaconda3-5.1.0-Linux-x86_64.sh  train.log\n",
            "compute-bleu.py\t\t\t UN.en-fr.en\n",
            "compute-bleu.py.1\t\t UN.en-fr.en-filtered.en\n",
            "compute-bleu.py.2\t\t UN.en-fr.en-filtered.en.subword\n",
            "compute-bleu.py.3\t\t UN.en-fr.en-filtered.en.subword.dev\n",
            "compute-bleu.py.4\t\t UN.en-fr.en-filtered.en.subword.test\n",
            "condacolab_install.log\t\t UN.en-fr.en-filtered.en.subword.test.desubword\n",
            "config.yaml\t\t\t UN.en-fr.en-filtered.en.subword.train\n",
            "en-fr.txt.zip\t\t\t UN.en-fr.fr\n",
            "en-fr.txt.zip.1\t\t\t UN.en-fr.fr-filtered.fr\n",
            "models\t\t\t\t UN.en-fr.fr-filtered.fr.subword\n",
            "MT-Preparation\t\t\t UN.en-fr.fr-filtered.fr.subword.dev\n",
            "nmt\t\t\t\t UN.en-fr.fr-filtered.fr.subword.test\n",
            "README\t\t\t\t UN.en-fr.fr-filtered.fr.subword.train\n",
            "run\t\t\t\t UN.en.translated\n",
            "source.model\t\t\t UN.en.translated.desubword\n",
            "target.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Training Configuration File\n",
        "\n",
        "The following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values: \n",
        "* `train_steps` - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option `early_stopping` can help stop the training when there is no considerable improvement.\n",
        "* `valid_steps` - 10000 can be good if the value `train_steps` is big enough. \n",
        "* `warmup_steps` - obviously, its value must be less than `train_steps`. Try 4000 and 8000 values.\n",
        "\n",
        "Refer to [OpenNMT-py training parameters](https://opennmt.net/OpenNMT-py/options/train.html) for more details. If you are interested in further explanation of the Transformer model, you can check this article, [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)."
      ],
      "metadata": {
        "id": "ikohKcb0eWpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the YAML configuration file\n",
        "# On a regular machine, you can create it manually or with nano\n",
        "# Note here we are using some smaller values because the dataset is small\n",
        "# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n",
        "\n",
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: UN.en-fr.fr-filtered.fr.subword.train\n",
        "        path_tgt: UN.en-fr.en-filtered.en.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: UN.en-fr.fr-filtered.fr.subword.dev\n",
        "        path_tgt: UN.en-fr.en-filtered.en.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.fren\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps \n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 3000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adagrad\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 1024\n",
        "word_vec_size: 1024\n",
        "transformer_ff: 4096\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "_--WekqjeXpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Optional] Check the content of the configuration file\n",
        "!cat config.yaml"
      ],
      "metadata": {
        "id": "v6PyNDujebnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d85927-2dc8-437d-9fea-f51ba4ba82f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# config.yaml\n",
            "\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: run\n",
            "\n",
            "# Training files\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: UN.en-fr.fr-filtered.fr.subword.train\n",
            "        path_tgt: UN.en-fr.en-filtered.en.subword.train\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: UN.en-fr.fr-filtered.fr.subword.dev\n",
            "        path_tgt: UN.en-fr.en-filtered.en.subword.dev\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabulary files, generated by onmt_build_vocab\n",
            "src_vocab: run/source.vocab\n",
            "tgt_vocab: run/target.vocab\n",
            "\n",
            "# Vocabulary size - should be the same as in sentence piece\n",
            "src_vocab_size: 50000\n",
            "tgt_vocab_size: 50000\n",
            "\n",
            "# Filter out source/target longer than n if [filtertoolong] enabled\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenization options\n",
            "src_subword_model: source.model\n",
            "tgt_subword_model: target.model\n",
            "\n",
            "# Where to save the log file and the output models/checkpoints\n",
            "log_file: train.log\n",
            "save_model: models/model.fren\n",
            "\n",
            "# Stop training if it does not imporve after n validations\n",
            "early_stopping: 4\n",
            "\n",
            "# Default: 5000 - Save a model checkpoint for each n\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# To save space, limit checkpoints to last n\n",
            "# keep_checkpoint: 3\n",
            "\n",
            "seed: 3435\n",
            "\n",
            "# Default: 100000 - Train the model to max n steps \n",
            "# Increase to 200000 or more for large datasets\n",
            "# For fine-tuning, add up the required steps to the original steps\n",
            "train_steps: 3000\n",
            "\n",
            "# Default: 10000 - Run validation after n steps\n",
            "valid_steps: 1000\n",
            "\n",
            "# Default: 4000 - for large datasets, try up to 8000\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Number of GPUs, and IDs of GPUs\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Optimization\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adagrad\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Model\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 1024\n",
            "word_vec_size: 1024\n",
            "transformer_ff: 4096\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocabulary, Train Model, and Tune\n",
        "\n",
        "For large datasets, it is not feasable to use all words/tokens found in the corpus. Instead, build a specific vocabulary set from the training dataset, usually betweeen 32k and 100k words. "
      ],
      "metadata": {
        "id": "McArZKjJegaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of CPUs/cores on the machine\n",
        "!nproc --all"
      ],
      "metadata": {
        "id": "EeIffGjxed_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed47722-6763-4dec-8073-81a1e619f74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Vocabulary\n",
        "\n",
        "# -config: path to your config.yaml file\n",
        "# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n",
        "# -num_threads: change it to match the number of CPUs to run it faster\n",
        "\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "id": "CBKgwYkdejJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1930f173-cd61-4d46-e390-06f946b1ea08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-04-22 03:12:07,426 INFO] Counter vocab from -1 samples.\n",
            "[2023-04-22 03:12:07,426 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2023-04-22 03:12:12,576 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2010)\n",
            "\n",
            "[2023-04-22 03:12:13,329 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1979)\n",
            "\n",
            "[2023-04-22 03:12:13,377 INFO] Counters src: 14736\n",
            "[2023-04-22 03:12:13,377 INFO] Counters tgt: 11897\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/build_vocab.py\", line 196, in main\n",
            "    build_vocab_main(opts)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/build_vocab.py\", line 180, in build_vocab_main\n",
            "    save_counter(src_counter, opts.src_vocab)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/build_vocab.py\", line 169, in save_counter\n",
            "    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/utils/misc.py\", line 47, in check_path\n",
            "    raise IOError(f\"path {path} exists, stop.\")\n",
            "OSError: path run/source.vocab exists, stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the **Runtime menu** > **Change runtime type**, make sure that the \"**Hardware accelerator**\" is \"**GPU**\"."
      ],
      "metadata": {
        "id": "vhld0gaYetIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the GPU is active\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "N1xqNtXpenLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6c8dff-cd28-4e06-9df0-e568a25a50da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-df6bf92f-cacb-b40c-a7e1-b9c5ce3b5293)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"
      ],
      "metadata": {
        "id": "NVuX161cevFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccc1a61-77f3-49dc-c594-cb8f158bcd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14998.8125 out of: 15101.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf drive/MyDrive/nmt/models/"
      ],
      "metadata": {
        "id": "38WCCKohiC9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the NMT model\n",
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "id": "EhXKXrNTiK-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6483ed-c4a4-43be-b812-478b07d38068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-04-22 03:12:19,023 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-04-22 03:12:19,601 INFO] Parsed 2 corpora from -data.\n",
            "[2023-04-22 03:12:19,602 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2023-04-22 03:12:20,941 INFO] Building model...\n",
            "[2023-04-22 03:12:27,290 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(14720, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(11896, 1024, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (w_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=1024, out_features=11896, bias=True)\n",
            ")\n",
            "[2023-04-22 03:12:27,393 INFO] encoder: 90628096\n",
            "[2023-04-22 03:12:27,393 INFO] decoder: 125107832\n",
            "[2023-04-22 03:12:27,393 INFO] * number of parameters: 215735928\n",
            "[2023-04-22 03:12:27,393 INFO]  * src vocab size = 14720\n",
            "[2023-04-22 03:12:27,393 INFO]  * tgt vocab size = 11896\n",
            "[2023-04-22 03:12:27,408 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2023-04-22 03:12:27,409 INFO] Starting training on GPU: [0]\n",
            "[2023-04-22 03:12:27,409 INFO] Start training loop and validate every 1000 steps...\n",
            "[2023-04-22 03:12:27,409 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "[2023-04-22 03:12:34,477 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2023-04-22 03:12:41,004 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2023-04-22 03:12:47,938 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2023-04-22 03:12:54,478 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2023-04-22 03:15:52,825 INFO] Step 100/ 3000; acc: 7.7; ppl: 2302.2; xent: 7.7; lr: 0.00020; sents:   27244; bsz: 3986/3590/68; 7762/6991 tok/s;    205 sec;\n",
            "[2023-04-22 03:18:26,701 INFO] Step 200/ 3000; acc: 15.3; ppl: 558.6; xent: 6.3; lr: 0.00040; sents:   24181; bsz: 4004/3594/60; 10409/9343 tok/s;    359 sec;\n",
            "[2023-04-22 03:20:59,902 INFO] Step 300/ 3000; acc: 24.9; ppl: 253.7; xent: 5.5; lr: 0.00059; sents:   25561; bsz: 3996/3587/64; 10434/9366 tok/s;    512 sec;\n",
            "[2023-04-22 03:23:33,133 INFO] Step 400/ 3000; acc: 30.2; ppl: 160.7; xent: 5.1; lr: 0.00079; sents:   25759; bsz: 4000/3591/64; 10442/9375 tok/s;    666 sec;\n",
            "[2023-04-22 03:26:06,292 INFO] Step 500/ 3000; acc: 33.9; ppl: 116.3; xent: 4.8; lr: 0.00099; sents:   27320; bsz: 3991/3589/68; 10424/9373 tok/s;    819 sec;\n",
            "[2023-04-22 03:28:38,972 INFO] Step 600/ 3000; acc: 36.6; ppl:  93.7; xent: 4.5; lr: 0.00119; sents:   26921; bsz: 3984/3588/67; 10437/9399 tok/s;    972 sec;\n",
            "[2023-04-22 03:31:10,657 INFO] Step 700/ 3000; acc: 39.3; ppl:  77.2; xent: 4.3; lr: 0.00139; sents:   29369; bsz: 3982/3594/73; 10500/9477 tok/s;   1123 sec;\n",
            "[2023-04-22 03:33:44,764 INFO] Step 800/ 3000; acc: 41.1; ppl:  66.2; xent: 4.2; lr: 0.00158; sents:   24387; bsz: 4003/3586/61; 10390/9309 tok/s;   1277 sec;\n",
            "[2023-04-22 03:36:09,605 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18812)\n",
            "\n",
            "[2023-04-22 03:36:09,605 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2023-04-22 03:36:15,870 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2023-04-22 03:36:23,909 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2023-04-22 03:36:35,201 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2023-04-22 03:36:39,692 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2023-04-22 03:37:21,973 INFO] Step 900/ 3000; acc: 43.7; ppl:  56.2; xent: 4.0; lr: 0.00178; sents:   26220; bsz: 3997/3592/66; 7362/6614 tok/s;   1495 sec;\n",
            "[2023-04-22 03:39:56,145 INFO] Step 1000/ 3000; acc: 47.0; ppl:  45.8; xent: 3.8; lr: 0.00198; sents:   25819; bsz: 3993/3588/65; 10361/9309 tok/s;   1649 sec;\n",
            "[2023-04-22 03:40:04,476 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 8.327413320541382 s.\n",
            "[2023-04-22 03:40:04,478 INFO] Train perplexity: 154.043\n",
            "[2023-04-22 03:40:04,479 INFO] Train accuracy: 31.9759\n",
            "[2023-04-22 03:40:04,479 INFO] Sentences processed: 262781\n",
            "[2023-04-22 03:40:04,479 INFO] Average bsz: 3994/3590/66\n",
            "[2023-04-22 03:40:04,479 INFO] Validation perplexity: 39.3249\n",
            "[2023-04-22 03:40:04,479 INFO] Validation accuracy: 50.0066\n",
            "[2023-04-22 03:40:04,479 INFO] Model is improving ppl: inf --> 39.3249.\n",
            "[2023-04-22 03:40:04,479 INFO] Model is improving acc: -inf --> 50.0066.\n",
            "[2023-04-22 03:40:04,494 INFO] Saving checkpoint models/model.fren_step_1000.pt\n",
            "[2023-04-22 03:42:59,951 INFO] Step 1100/ 3000; acc: 50.6; ppl:  37.4; xent: 3.6; lr: 0.00188; sents:   25084; bsz: 3997/3583/63; 8699/7796 tok/s;   1833 sec;\n",
            "[2023-04-22 03:45:32,803 INFO] Step 1200/ 3000; acc: 54.5; ppl:  30.7; xent: 3.4; lr: 0.00180; sents:   28243; bsz: 3987/3594/71; 10434/9405 tok/s;   1985 sec;\n",
            "[2023-04-22 03:48:06,349 INFO] Step 1300/ 3000; acc: 57.6; ppl:  25.9; xent: 3.3; lr: 0.00173; sents:   26811; bsz: 3991/3591/67; 10398/9354 tok/s;   2139 sec;\n",
            "[2023-04-22 03:50:40,252 INFO] Step 1400/ 3000; acc: 59.9; ppl:  22.9; xent: 3.1; lr: 0.00167; sents:   25443; bsz: 4002/3588/64; 10401/9326 tok/s;   2293 sec;\n",
            "[2023-04-22 03:53:14,726 INFO] Step 1500/ 3000; acc: 62.2; ppl:  20.4; xent: 3.0; lr: 0.00161; sents:   25330; bsz: 3999/3593/63; 10356/9303 tok/s;   2447 sec;\n",
            "[2023-04-22 03:55:48,023 INFO] Step 1600/ 3000; acc: 64.9; ppl:  17.8; xent: 2.9; lr: 0.00156; sents:   27459; bsz: 3988/3591/69; 10406/9369 tok/s;   2601 sec;\n",
            "[2023-04-22 03:58:21,757 INFO] Step 1700/ 3000; acc: 66.5; ppl:  16.6; xent: 2.8; lr: 0.00152; sents:   25951; bsz: 3996/3589/65; 10396/9338 tok/s;   2754 sec;\n",
            "[2023-04-22 04:00:42,731 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18726)\n",
            "\n",
            "[2023-04-22 04:00:42,731 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2023-04-22 04:00:46,960 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2023-04-22 04:00:51,056 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2023-04-22 04:01:00,862 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2023-04-22 04:01:04,845 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2023-04-22 04:01:53,591 INFO] Step 1800/ 3000; acc: 68.5; ppl:  15.0; xent: 2.7; lr: 0.00147; sents:   27257; bsz: 3992/3597/68; 7538/6792 tok/s;   2966 sec;\n",
            "[2023-04-22 04:04:26,957 INFO] Step 1900/ 3000; acc: 70.2; ppl:  13.8; xent: 2.6; lr: 0.00143; sents:   27282; bsz: 3992/3597/68; 10411/9381 tok/s;   3120 sec;\n",
            "[2023-04-22 04:06:59,935 INFO] Step 2000/ 3000; acc: 71.2; ppl:  13.2; xent: 2.6; lr: 0.00140; sents:   27883; bsz: 3989/3589/70; 10429/9384 tok/s;   3273 sec;\n",
            "[2023-04-22 04:07:00,315 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=131)\n",
            "\n",
            "[2023-04-22 04:07:08,329 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 8.390708923339844 s.\n",
            "[2023-04-22 04:07:08,333 INFO] Train perplexity: 55.7265\n",
            "[2023-04-22 04:07:08,334 INFO] Train accuracy: 47.2948\n",
            "[2023-04-22 04:07:08,334 INFO] Sentences processed: 529524\n",
            "[2023-04-22 04:07:08,334 INFO] Average bsz: 3993/3590/66\n",
            "[2023-04-22 04:07:08,334 INFO] Validation perplexity: 13.3148\n",
            "[2023-04-22 04:07:08,334 INFO] Validation accuracy: 71.8075\n",
            "[2023-04-22 04:07:08,334 INFO] Model is improving ppl: 39.3249 --> 13.3148.\n",
            "[2023-04-22 04:07:08,334 INFO] Model is improving acc: 50.0066 --> 71.8075.\n",
            "[2023-04-22 04:07:08,353 INFO] Saving checkpoint models/model.fren_step_2000.pt\n",
            "[2023-04-22 04:10:04,151 INFO] Step 2100/ 3000; acc: 72.0; ppl:  12.6; xent: 2.5; lr: 0.00136; sents:   25723; bsz: 3998/3587/64; 8680/7788 tok/s;   3457 sec;\n",
            "[2023-04-22 04:12:39,136 INFO] Step 2200/ 3000; acc: 72.9; ppl:  12.1; xent: 2.5; lr: 0.00133; sents:   25468; bsz: 4000/3585/64; 10323/9252 tok/s;   3612 sec;\n",
            "[2023-04-22 04:15:13,285 INFO] Step 2300/ 3000; acc: 73.7; ppl:  11.6; xent: 2.5; lr: 0.00130; sents:   25516; bsz: 3996/3586/64; 10370/9305 tok/s;   3766 sec;\n",
            "[2023-04-22 04:17:47,753 INFO] Step 2400/ 3000; acc: 74.7; ppl:  11.1; xent: 2.4; lr: 0.00128; sents:   25568; bsz: 3998/3586/64; 10354/9286 tok/s;   3920 sec;\n",
            "[2023-04-22 04:20:21,572 INFO] Step 2500/ 3000; acc: 75.5; ppl:  10.7; xent: 2.4; lr: 0.00125; sents:   25681; bsz: 3991/3593/64; 10377/9342 tok/s;   4074 sec;\n",
            "[2023-04-22 04:22:54,957 INFO] Step 2600/ 3000; acc: 76.5; ppl:  10.3; xent: 2.3; lr: 0.00123; sents:   27654; bsz: 3989/3595/69; 10402/9376 tok/s;   4228 sec;\n",
            "[2023-04-22 04:25:10,888 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18759)\n",
            "\n",
            "[2023-04-22 04:25:10,888 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2023-04-22 04:25:21,006 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2023-04-22 04:25:25,734 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2023-04-22 04:25:36,638 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2023-04-22 04:26:36,204 INFO] Step 2700/ 3000; acc: 76.9; ppl:  10.0; xent: 2.3; lr: 0.00120; sents:   26616; bsz: 3989/3592/67; 7211/6494 tok/s;   4449 sec;\n",
            "[2023-04-22 04:29:10,071 INFO] Step 2800/ 3000; acc: 77.7; ppl:   9.6; xent: 2.3; lr: 0.00118; sents:   26122; bsz: 3988/3591/65; 10368/9334 tok/s;   4603 sec;\n",
            "[2023-04-22 04:31:44,043 INFO] Step 2900/ 3000; acc: 77.9; ppl:   9.5; xent: 2.3; lr: 0.00116; sents:   26435; bsz: 3994/3588/66; 10376/9322 tok/s;   4757 sec;\n",
            "[2023-04-22 04:34:18,409 INFO] Step 3000/ 3000; acc: 78.5; ppl:   9.3; xent: 2.2; lr: 0.00114; sents:   25405; bsz: 4002/3592/64; 10371/9308 tok/s;   4911 sec;\n",
            "[2023-04-22 04:34:18,725 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=131)\n",
            "\n",
            "[2023-04-22 04:34:26,753 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 8.340246677398682 s.\n",
            "[2023-04-22 04:34:26,755 INFO] Train perplexity: 32.0679\n",
            "[2023-04-22 04:34:26,756 INFO] Train accuracy: 56.7374\n",
            "[2023-04-22 04:34:26,756 INFO] Sentences processed: 789712\n",
            "[2023-04-22 04:34:26,756 INFO] Average bsz: 3994/3590/66\n",
            "[2023-04-22 04:34:26,756 INFO] Validation perplexity: 9.9477\n",
            "[2023-04-22 04:34:26,756 INFO] Validation accuracy: 78.2629\n",
            "[2023-04-22 04:34:26,756 INFO] Model is improving ppl: 13.3148 --> 9.9477.\n",
            "[2023-04-22 04:34:26,756 INFO] Model is improving acc: 71.8075 --> 78.2629.\n",
            "[2023-04-22 04:34:26,775 INFO] Saving checkpoint models/model.fren_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'sgd', 'adagrad', 'adadelta', 'adam', 'sparseadam', 'adafactor', 'fusedadam'"
      ],
      "metadata": {
        "id": "5ww2XTURjLpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model and Results\n",
        "\n",
        "Translation Options:\n",
        "* `-model` - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n",
        "* `-src` - the subworded test dataset, source file\n",
        "* `-output` - give any file name to the new translation output file\n",
        "* `-gpu` - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n",
        "* `-min_length` - [optional] to avoid empty translations\n",
        "* `-verbose` - [optional] if you want to print translations\n",
        "\n",
        "Refer to [OpenNMT-py translation options](https://opennmt.net/OpenNMT-py/options/translate.html) for more details."
      ],
      "metadata": {
        "id": "QjLFU15xr8z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate the \"subworded\" source file of the test dataset\n",
        "# Change the model name, if needed.\n",
        "!onmt_translate -model models/model.fren_step_3000.pt -src UN.en-fr.fr-filtered.fr.subword.test -output UN.en.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "id": "26Y6v3nUiMgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4829e0-b303-47f2-f3ee-d47346a5f5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_translate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/translate.py\", line 60, in main\n",
            "    translate(opt)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/translate.py\", line 41, in translate\n",
            "    _, _ = translator._translate(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/translate/translator.py\", line 353, in _translate\n",
            "    batch_data = self.translate_batch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/translate/translator.py\", line 801, in translate_batch\n",
            "    return self._translate_batch_with_strategy(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/translate/translator.py\", line 868, in _translate_batch_with_strategy\n",
            "    ) = decode_strategy.initialize(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/translate/beam_search.py\", line 343, in initialize\n",
            "    super(BeamSearch, self).initialize_(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/translate/beam_search.py\", line 107, in initialize_\n",
            "    self.topk_log_probs = torch.tensor(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first 5 lines of the translation file\n",
        "!head -n 5 UN.en.translated"
      ],
      "metadata": {
        "id": "3daJ0JwJsCa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed install/update sentencepiece\n",
        "!pip3 install --upgrade -q sentencepiece\n",
        "\n",
        "# Desubword the translation file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en.translated"
      ],
      "metadata": {
        "id": "do-yK66hscKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b46ba1f-69d5-4f27-c065-61704c065dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "\n",
            "^C\n",
            "Done desubwording! Output: UN.en.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first 5 lines of the desubworded translation file\n",
        "!head -n 5 UN.en.translated.desubword"
      ],
      "metadata": {
        "id": "bPGk-6a5sdo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14347b9b-ddf9-498b-be91-407189f1d68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16. Calls upon UN-Habitat to intensify efforts to coordinate and implement operational activities for operational activities and operational activities for operational activities, and invites all countries to provide a position to do so to support the activities of UN-Habitat in this regard;\n",
            "7. Welcomes the convening of international response to marine biodiversity and the Food and Agriculture Organization of the United Nations, recognizes the role of the Food and Agriculture Organization of the United Nations, which should be in accordance with international law, including the relevant agreements of the World Trade Organization, and notes that the ongoing United Nations in this regard;\n",
            "Having considered the report of the Secretary-General on the financing of the United Nations Operation in Côte d'IvoireA/61/468. and the related report of the Advisory Committee on Administrative and Budgetary Questions,A/61/551.\n",
            "1. Takes note of the report of the Secretary-General;A/57/177.\n",
            "(c) The elements of information concerning arbitrary arrest and arbitrary arrest and detention, including those events;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Desubword the target file (reference) of the test dataset\n",
        "# Note: You might as well have split files *before* subwording during dataset preperation, \n",
        "# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en-fr.en-filtered.en.subword.test"
      ],
      "metadata": {
        "id": "SGfBq0F0sfLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68903dd0-ad5c-4b1d-e6c2-244d0622fa6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: UN.en-fr.en-filtered.en.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first 5 lines of the desubworded reference\n",
        "!head -n 5 UN.en-fr.en-filtered.en.subword.test.desubword"
      ],
      "metadata": {
        "id": "VQviYhqBsgJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c26314-630d-406a-e7a5-7cb4032187ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16. Calls upon UN-Habitat to strengthen efforts to coordinate and implement its normative and operational activities through the enhanced normative and operational framework elaborated in the medium-term strategic and institutional plan, reinforcing its normative activities, and invites all countries in a position to do so to support the activities of UN-Habitat in this regard;\n",
            "7. Welcomes the 2005 International Guidelines for the Ecolabelling of Fish and Fishery Products from Marine Capture Fisheries of the Food and Agriculture Organization of the United Nations, acknowledges the role of certification and ecolabelling schemes, which are to be consistent with international law, including relevant World Trade Organization agreements, and notes ongoing discussions in the World Trade Organization on such schemes;\n",
            "Having considered the report of the Secretary-General on the financing of the United Nations Operation in Côte d'IvoireA/61/468. and the related report of the Advisory Committee on Administrative and Budgetary Questions,A/61/551.\n",
            "1. Takes note of the report of the Secretary-General;A/57/177.\n",
            "(c) Reports of arbitrary arrest and detention, including of eyewitnesses to the events in Andijan;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "We are using BLEU for evaluation. Files must be detokenized/desubworded beforehand."
      ],
      "metadata": {
        "id": "PexKBYJAshdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the BLEU script\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"
      ],
      "metadata": {
        "id": "qLFBpXHvsjb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb51064-1e14-4efa-c0d2-11830c68373e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-22 04:38:37--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957 [text/plain]\n",
            "Saving to: ‘compute-bleu.py.5’\n",
            "\n",
            "compute-bleu.py.5   100%[===================>]     957  --.-KB/s    in 0s      \n",
            "\n",
            "2023-04-22 04:38:38 (30.9 MB/s) - ‘compute-bleu.py.5’ saved [957/957]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sacrebleu\n",
        "!pip3 install sacrebleu"
      ],
      "metadata": {
        "id": "J9Ma75VuskgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95de67bd-0aae-4193-8f24-25d8b6cb72af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.9/dist-packages (2.3.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (1.22.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (2022.10.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the translation (without subwording)\n",
        "!python3 compute-bleu.py UN.en-fr.en-filtered.en.subword.test.desubword UN.en.translated.desubword"
      ],
      "metadata": {
        "id": "QuxRMM81sl50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99460117-bb20-4142-dc3b-3b1ef378202d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference 1st sentence: 16. Calls upon UN-Habitat to strengthen efforts to coordinate and implement its normative and operational activities through the enhanced normative and operational framework elaborated in the medium-term strategic and institutional plan, reinforcing its normative activities, and invites all countries in a position to do so to support the activities of UN-Habitat in this regard;\n",
            "MTed 1st sentence: 16. Calls upon UN-Habitat to intensify efforts to coordinate and implement operational activities for operational activities and operational activities for operational activities, and invites all countries to provide a position to do so to support the activities of UN-Habitat in this regard;\n",
            "BLEU:  48.6884000325136\n"
          ]
        }
      ]
    }
  ]
}