{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T9GpLuAvIwtT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirahg/Translation-Model/blob/main/Translation_Model_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DS4420 Final Project: English-French Translation Model**\n",
        "## Mirah Gordon and Jeremy Cui\n",
        "### Adam Optimizer"
      ],
      "metadata": {
        "id": "O6NMpMnBV5S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XMk-V8nT9IpU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLRpsedwqLg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3defd745-7d45-44cf-e3c7-5869f3095933"
      },
      "source": [
        "# Create a directory and clone the Github MT-Preparation repository\n",
        "!mkdir -p nmt\n",
        "%cd nmt\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt\n",
            "Cloning into 'MT-Preparation'...\n",
            "remote: Enumerating objects: 227, done.\u001b[K\n",
            "remote: Counting objects: 100% (227/227), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 227 (delta 115), reused 183 (delta 94), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (227/227), 54.63 KiB | 2.38 MiB/s, done.\n",
            "Resolving deltas: 100% (115/115), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8d13pqsp3Ii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee18972-dfa4-494c-b308-129e6943b89c"
      },
      "source": [
        "# Install the requirements\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (1.5.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G903Vcm7u08U"
      },
      "source": [
        "# Raw Data\n",
        "\n",
        "English-French Dataset:\n",
        "\n",
        "* EN-FR: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WmiX_xTqqdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d26d441-6284-444d-b578-0b5df5d78351"
      },
      "source": [
        "# Download and unzip a dataset\n",
        "!wget https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
        "!unzip en-fr.txt.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-21 20:24:59--  https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10014972 (9.6M) [application/zip]\n",
            "Saving to: ‘en-fr.txt.zip’\n",
            "\n",
            "en-fr.txt.zip       100%[===================>]   9.55M  7.02MB/s    in 1.4s    \n",
            "\n",
            "2023-04-21 20:25:01 (7.02 MB/s) - ‘en-fr.txt.zip’ saved [10014972/10014972]\n",
            "\n",
            "Archive:  en-fr.txt.zip\n",
            "  inflating: UN.en-fr.en             \n",
            "  inflating: UN.en-fr.fr             \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Filtering\n",
        "\n",
        "Filtering out low-quality segments can help improve the translation quality of the output MT model. This might include misalignments, empty segments, duplicates, among other issues. "
      ],
      "metadata": {
        "id": "5G6GTlXa86Qb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9jDIWarB-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b836a1d-2ac0-4617-bdfa-724c5d7678b7"
      },
      "source": [
        "# Filter the dataset\n",
        "# Arguments: source file, target file, source language, target language\n",
        "!python3 MT-Preparation/filtering/filter.py UN.en-fr.fr UN.en-fr.en fr en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape (rows, columns): (74067, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 74067\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 60662\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 60476\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 59719\n",
            "--- HTML Removed\t\t\t--> Rows: 59719\n",
            "--- Rows will remain in true-cased\t--> Rows: 59719\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 59719\n",
            "--- Rows Shuffled\t\t\t--> Rows: 59719\n",
            "--- Source Saved: UN.en-fr.fr-filtered.fr\n",
            "--- Target Saved: UN.en-fr.en-filtered.en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization / Sub-wording\n",
        "\n",
        "To build a vocabulary for any NLP model, you have to tokenize (i.e. split) sentences into smaller units. Word-based tokenization used to be the way to go; in this case, each word would be a token. However, an MT model can only learn a specific number of vocabulary tokens due to limited hardware resources. To solve this issue, sub-words are used instead of whole words. At translation time, when the model sees a new word/token that looks like a word/token it has in the vocabulary, it still can try to continue the translation instead of marking this word as “unknown” or “unk”.\n",
        "\n",
        "There are a few approaches to sub-wording such as BPE and the unigram model. One of the famous toolkits that incorporates the most common approaches is [SentencePiece](https://github.com/google/sentencepiece). Note that you have to train a sub-wording model and then use it. After translation, you will have to “desubword” or “decode” your text back using the same SentencePiece model.\n",
        "\n"
      ],
      "metadata": {
        "id": "IbRpxXjC78c0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9c1pqhuru3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa4df11-3102-4e8b-a941-6a74cee14aed"
      },
      "source": [
        "!ls MT-Preparation/subwording/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-train_bpe.py\t1-train_unigram.py  2-subword.py  3-desubword.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weSS6QDPsOUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02c2784-a6a0-4527-c63e-aadce45e2e28"
      },
      "source": [
        "# Train a SentencePiece model for subword tokenization\n",
        "!python3 MT-Preparation/subwording/1-train_unigram.py UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=UN.en-fr.fr-filtered.fr --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: UN.en-fr.fr-filtered.fr\n",
            "  input_format: \n",
            "  model_prefix: source\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: UN.en-fr.fr-filtered.fr\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 59719 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=19614832\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9546% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=82\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999546\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 59719 sentences.\n",
            "unigram_model_trainer.cc(247) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(251) LOG(INFO) Extracting frequent sub strings... node_num=13633421\n",
            "unigram_model_trainer.cc(301) LOG(INFO) Initialized 74218 seed sentencepieces\n",
            "unigram_model_trainer.cc(150) [!std::isnan(score)] \n",
            "Program terminated with an unrecoverable error.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T89THXeRslKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8538fb46-9b67-4fde-a3f8-6a4144153a2a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-fr.txt.zip\tREADME\t     UN.en-fr.en-filtered.en  UN.en-fr.fr-filtered.fr\n",
            "MT-Preparation\tUN.en-fr.en  UN.en-fr.fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBWQoCfBsqlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5c854c-58f3-4d9f-8163-113473b06ecb"
      },
      "source": [
        "# Subword the dataset\n",
        "!python3 MT-Preparation/subwording/2-subword.py source.model target.model UN.en-fr.fr-filtered.fr UN.en-fr.en-filtered.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: UN.en-fr.fr-filtered.fr\n",
            "Target Dataset: UN.en-fr.en-filtered.en\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nmt/MT-Preparation/subwording/2-subword.py\", line 30, in <module>\n",
            "    sp.load(source_model)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sentencepiece/__init__.py\", line 905, in Load\n",
            "    return self.LoadFromFile(model_file)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sentencepiece/__init__.py\", line 310, in LoadFromFile\n",
            "    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\n",
            "OSError: Not found: \"source.model\": No such file or directory Error #2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnfMRckbvNfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8dc5aa7-23ff-403a-e218-50d29c410a40"
      },
      "source": [
        "# First 3 lines before subwording\n",
        "!head -n 3 UN.en-fr.fr-filtered.fr && echo \"-----\" && head -n 3 UN.en-fr.en-filtered.en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6. Encourage les États Membres à ratifier la Convention internationale contre le dopage dans le sportOrganisation des Nations Unies pour l'éducation, la science et la culture, Actes de la Conférence générale, trente-troisième session, Paris, 3-21 octobre 2005, vol. I : Résolutions, chap. V, résolution 14. ;\n",
            "Rappelant la Déclaration de HyogoA/CONF.206/6, chap. I, résolution 1. et le Cadre d'action de Hyogo pour 2005-2015Cadre d'action de Hyogo pour 2005-2015 : Pour des nations et des collectivités résilientes face aux catastrophes (A/CONF.206/6, chap. I, résolution 2)., ainsi que la déclaration commune publiée à l'issue de la session extraordinaire sur la catastrophe de l'océan IndienDéclaration commune de la session extraordinaire consacrée à la catastrophe de l'océan Indien : réduction des risques pour un avenir plus sûr (A/CONF.206/6, annexe II)., adoptés à la Conférence mondiale sur la prévention des catastrophes tenue à Kobe, dans la préfecture de Hyogo (Japon), du 18 au 22 janvier 2005,\n",
            "1. Décide de créer le Bureau du Haut Représentant pour les pays les moins avancés, les pays en développement sans littoral et les petits États insulaires en développement, qui exercera les fonctions recommandées par le Secrétaire général dans son rapportA/56/645, par. 17. ;\n",
            "-----\n",
            "6. Encourages Member States to ratify the International Convention Against Doping in Sport;United Nations Educational, Scientific and Cultural Organization, Records of the General Conference, Thirty-third Session, Paris, 3-21 October 2005, vol. 1: Resolutions, chap. V, resolution 14.\n",
            "Recalling the Hyogo DeclarationA/CONF.206/6 and Corr.1, chap. I, resolution 1. and the Hyogo Framework for Action 2005-2015,Hyogo Framework for Action 2005-2015: Building the Resilience of Nations and Communities to Disasters (A/CONF.206/6 and Corr.1, chap. I, resolution 2). as well as the common statement of the special session on the Indian Ocean disaster,Common statement of the special session on the Indian Ocean disaster: risk reduction for a safer future (A/CONF.206/6 and Corr.1, annex II). adopted at the World Conference on Disaster Reduction, held in Kobe, Hyogo, Japan, from 18 to 22 January 2005,\n",
            "1. Decides to establish the Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States, having the functions recommended by the Secretary-General in his report;A/56/645, para. 17.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs_xxKK_vf1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c188a2-8572-410e-9f4d-ad732b77b666"
      },
      "source": [
        "# First 3 lines after subwording\n",
        "!head -n 3 UN.en-fr.fr-filtered.fr.subword && echo \"---\" && head -n 3 UN.en-fr.en-filtered.en.subword"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open 'UN.en-fr.fr-filtered.fr.subword' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting\n",
        "\n",
        "Split the dataset into 3 portions:\n",
        "\n",
        "1. training dataset - used for training the model;\n",
        "2. development dataset - used to run regular validations during the training to help improve the model parameters; and\n",
        "3. testing dataset - a holdout dataset used after the model finishes training to finally evaluate the model on unseen data."
      ],
      "metadata": {
        "id": "YgTZ-m718neI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfQRMGRixBAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db6f5c8-f55d-46b8-9c4f-c9fb21a87bdb"
      },
      "source": [
        "# Split the dataset into training set, development set, and test set\n",
        "# here we chose 2000\n",
        "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000 UN.en-fr.fr-filtered.fr.subword UN.en-fr.en-filtered.en.subword"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/nmt/MT-Preparation/train_dev_split/train_dev_test_split.py\", line 106, in <module>\n",
            "    extract_dev(segment_no_dev, segment_no_test, source_file, target_file)\n",
            "  File \"/content/nmt/MT-Preparation/train_dev_split/train_dev_test_split.py\", line 26, in extract_dev\n",
            "    df_source = pd.read_csv(source_file,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 605, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\", line 856, in get_handle\n",
            "    handle = open(\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'UN.en-fr.fr-filtered.fr.subword'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y3HQr4nxYib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7dac71-e633-47d6-def0-c17f8680cf4b"
      },
      "source": [
        "# Line count for the subworded train, dev, test datatest\n",
        "!wc -l *.subword.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wc: '*.subword.*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0duUCLP93GKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f07f4a7-8099-4cd4-abfa-ed121331b277"
      },
      "source": [
        "# Check the first and last line from each dataset\n",
        "\n",
        "# -------------------------------------------\n",
        "# Change this cell to print your name\n",
        "!echo -e \"My name is: FirstName SecondName \\n\"\n",
        "# -------------------------------------------\n",
        "\n",
        "!echo \"---First line---\"\n",
        "!head -n 1 *.{train,dev,test}\n",
        "\n",
        "!echo -e \"\\n---Last line---\"\n",
        "!tail -n 1 *.{train,dev,test}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is: FirstName SecondName \n",
            "\n",
            "---First line---\n",
            "head: cannot open '*.train' for reading: No such file or directory\n",
            "head: cannot open '*.dev' for reading: No such file or directory\n",
            "head: cannot open '*.test' for reading: No such file or directory\n",
            "\n",
            "---Last line---\n",
            "tail: cannot open '*.train' for reading: No such file or directory\n",
            "tail: cannot open '*.dev' for reading: No such file or directory\n",
            "tail: cannot open '*.test' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9GpLuAvIwtT"
      },
      "source": [
        "# Mount your drive to save your data\n",
        "\n",
        "Click the folder icon to the left, and then click the Google Drive icon.\n",
        "\n",
        "![mount-drive.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnAr4M7ebcz3",
        "outputId": "d4301e7a-1a9c-43e5-d656-14f929477981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZjU1Lx8IxmS"
      },
      "source": [
        "# Copy your data to your Google Drive\n",
        "!cp -R /content/nmt/ /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OpenNMT-py 3.x\n",
        "!pip3 install OpenNMT-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfwkjGB3eJte",
        "outputId": "15964b5a-8f76-4bb0-8a6d-9fa930354937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.1.1-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/227.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.2/227.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.9/dist-packages (from OpenNMT-py) (2.2.3)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from OpenNMT-py) (6.0)\n",
            "Collecting waitress\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<2,>=1.13\n",
            "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.9/dist-packages (from OpenNMT-py) (2.12.2)\n",
            "Collecting ctranslate2<4,>=3.2\n",
            "  Downloading ctranslate2-3.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.7/31.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35\n",
            "  Downloading pyonmttok-1.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from ctranslate2<4,>=3.2->OpenNMT-py) (1.22.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.4.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (67.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.53.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.0)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<2,>=1.13->OpenNMT-py) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (3.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (6.4.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask->OpenNMT-py) (8.1.3)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu->OpenNMT-py) (2022.10.31)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu->OpenNMT-py) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu->OpenNMT-py) (0.8.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask->OpenNMT-py) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (1.26.15)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pyahocorasick, portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cudnn-cu11, torch, OpenNMT-py\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenNMT-py-3.1.1 colorama-0.4.6 configargparse-1.5.3 ctranslate2-3.12.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 pyahocorasick-2.0.0 pyonmttok-1.37.1 rapidfuzz-3.0.0 sacrebleu-2.3.1 torch-1.13.1 waitress-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Your Datasets"
      ],
      "metadata": {
        "id": "H5nXtsnReQT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the folder where you saved your prepapred datasets from the first exercise\n",
        "# You might need to mount your Google Drive first\n",
        "%cd /content/drive/MyDrive/nmt/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS9cjn_HeKIi",
        "outputId": "2ecacb0a-f7c5-4803-e0c7-1dcdf4676d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nmt\n",
            "Anaconda3-5.1.0-Linux-x86_64.sh  UN.en-fr.en-filtered.en\n",
            "condacolab_install.log\t\t UN.en-fr.en-filtered.en.subword\n",
            "config.yaml\t\t\t UN.en-fr.en-filtered.en.subword.dev\n",
            "en-fr.txt.zip\t\t\t UN.en-fr.en-filtered.en.subword.test\n",
            "MT-Preparation\t\t\t UN.en-fr.en-filtered.en.subword.train\n",
            "nmt\t\t\t\t UN.en-fr.fr\n",
            "README\t\t\t\t UN.en-fr.fr-filtered.fr\n",
            "run\t\t\t\t UN.en-fr.fr-filtered.fr.subword\n",
            "source.model\t\t\t UN.en-fr.fr-filtered.fr.subword.dev\n",
            "target.model\t\t\t UN.en-fr.fr-filtered.fr.subword.test\n",
            "UN.en-fr.en\t\t\t UN.en-fr.fr-filtered.fr.subword.train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Training Configuration File\n",
        "\n",
        "The following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values: \n",
        "* `train_steps` - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option `early_stopping` can help stop the training when there is no considerable improvement.\n",
        "* `valid_steps` - 10000 can be good if the value `train_steps` is big enough. \n",
        "* `warmup_steps` - obviously, its value must be less than `train_steps`. Try 4000 and 8000 values.\n",
        "\n",
        "Refer to [OpenNMT-py training parameters](https://opennmt.net/OpenNMT-py/options/train.html) for more details. If you are interested in further explanation of the Transformer model, you can check this article, [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)."
      ],
      "metadata": {
        "id": "ikohKcb0eWpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the YAML configuration file\n",
        "# On a regular machine, you can create it manually or with nano\n",
        "# Note here we are using some smaller values because the dataset is small\n",
        "# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n",
        "\n",
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: UN.en-fr.fr-filtered.fr.subword.train\n",
        "        path_tgt: UN.en-fr.en-filtered.en.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: UN.en-fr.fr-filtered.fr.subword.dev\n",
        "        path_tgt: UN.en-fr.en-filtered.en.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.fren\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps \n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 3000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "_--WekqjeXpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Optional] Check the content of the configuration file\n",
        "!cat config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6PyNDujebnm",
        "outputId": "35b42005-eb4a-4167-8c00-5b77a0907b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# config.yaml\n",
            "\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: run\n",
            "\n",
            "# Training files\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: UN.en-fr.fr-filtered.fr.subword.train\n",
            "        path_tgt: UN.en-fr.en-filtered.en.subword.train\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: UN.en-fr.fr-filtered.fr.subword.dev\n",
            "        path_tgt: UN.en-fr.en-filtered.en.subword.dev\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabulary files, generated by onmt_build_vocab\n",
            "src_vocab: run/source.vocab\n",
            "tgt_vocab: run/target.vocab\n",
            "\n",
            "# Vocabulary size - should be the same as in sentence piece\n",
            "src_vocab_size: 50000\n",
            "tgt_vocab_size: 50000\n",
            "\n",
            "# Filter out source/target longer than n if [filtertoolong] enabled\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenization options\n",
            "src_subword_model: source.model\n",
            "tgt_subword_model: target.model\n",
            "\n",
            "# Where to save the log file and the output models/checkpoints\n",
            "log_file: train.log\n",
            "save_model: models/model.fren\n",
            "\n",
            "# Stop training if it does not imporve after n validations\n",
            "early_stopping: 4\n",
            "\n",
            "# Default: 5000 - Save a model checkpoint for each n\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# To save space, limit checkpoints to last n\n",
            "# keep_checkpoint: 3\n",
            "\n",
            "seed: 3435\n",
            "\n",
            "# Default: 100000 - Train the model to max n steps \n",
            "# Increase to 200000 or more for large datasets\n",
            "# For fine-tuning, add up the required steps to the original steps\n",
            "train_steps: 3000\n",
            "\n",
            "# Default: 10000 - Run validation after n steps\n",
            "valid_steps: 1000\n",
            "\n",
            "# Default: 4000 - for large datasets, try up to 8000\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Number of GPUs, and IDs of GPUs\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Optimization\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adam\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Model\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 512\n",
            "word_vec_size: 512\n",
            "transformer_ff: 2048\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocabulary, Train Model, and Tune\n",
        "\n",
        "For large datasets, it is not feasable to use all words/tokens found in the corpus. Instead, build a specific vocabulary set from the training dataset, usually betweeen 32k and 100k words. "
      ],
      "metadata": {
        "id": "McArZKjJegaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of CPUs/cores on the machine\n",
        "!nproc --all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeIffGjxed_O",
        "outputId": "1ccbdfe2-fa40-4fcc-b211-49c6ba0b586b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Vocabulary\n",
        "\n",
        "# -config: path to your config.yaml file\n",
        "# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n",
        "# -num_threads: change it to match the number of CPUs to run it faster\n",
        "\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBKgwYkdejJG",
        "outputId": "2e604b4e-2d6a-4a3c-ebd9-71ab1394e787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-04-21 20:27:53,964 INFO] Counter vocab from -1 samples.\n",
            "[2023-04-21 20:27:53,965 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2023-04-21 20:28:01,590 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2119)\n",
            "\n",
            "[2023-04-21 20:28:01,688 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2062)\n",
            "\n",
            "[2023-04-21 20:28:01,744 INFO] Counters src: 14714\n",
            "[2023-04-21 20:28:01,744 INFO] Counters tgt: 11888\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/build_vocab.py\", line 196, in main\n",
            "    build_vocab_main(opts)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/build_vocab.py\", line 180, in build_vocab_main\n",
            "    save_counter(src_counter, opts.src_vocab)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/bin/build_vocab.py\", line 169, in save_counter\n",
            "    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/onmt/utils/misc.py\", line 47, in check_path\n",
            "    raise IOError(f\"path {path} exists, stop.\")\n",
            "OSError: path run/source.vocab exists, stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the **Runtime menu** > **Change runtime type**, make sure that the \"**Hardware accelerator**\" is \"**GPU**\"."
      ],
      "metadata": {
        "id": "vhld0gaYetIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the GPU is active\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1xqNtXpenLh",
        "outputId": "fa1ddb60-f492-43da-e969-357c0f261ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-ec5d8400-faca-d8c0-8ca1-2ae5dd96885f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVuX161cevFJ",
        "outputId": "03b09b95-67f2-45d2-f3c6-e1d4f902f649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14998.8125 out of: 15101.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf drive/MyDrive/nmt/models/"
      ],
      "metadata": {
        "id": "38WCCKohiC9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the NMT model\n",
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhXKXrNTiK-t",
        "outputId": "738cf961-0bbf-4665-8b7a-2709d32caff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-04-21 20:36:59,621 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-04-21 20:36:59,622 INFO] Parsed 2 corpora from -data.\n",
            "[2023-04-21 20:36:59,623 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2023-04-21 20:37:00,221 INFO] Building model...\n",
            "[2023-04-21 20:37:02,826 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(14720, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(11896, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=11896, bias=True)\n",
            ")\n",
            "[2023-04-21 20:37:02,883 INFO] encoder: 26439680\n",
            "[2023-04-21 20:37:02,883 INFO] decoder: 37394040\n",
            "[2023-04-21 20:37:02,883 INFO] * number of parameters: 63833720\n",
            "[2023-04-21 20:37:02,883 INFO]  * src vocab size = 14720\n",
            "[2023-04-21 20:37:02,883 INFO]  * tgt vocab size = 11896\n",
            "[2023-04-21 20:37:02,891 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2023-04-21 20:37:02,891 INFO] Starting training on GPU: [0]\n",
            "[2023-04-21 20:37:02,892 INFO] Start training loop and validate every 1000 steps...\n",
            "[2023-04-21 20:37:02,892 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "[2023-04-21 20:37:07,994 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2023-04-21 20:37:15,688 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2023-04-21 20:37:21,786 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2023-04-21 20:37:28,499 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2023-04-21 20:39:07,736 INFO] Step 100/ 3000; acc: 9.4; ppl: 1554.7; xent: 7.3; lr: 0.00028; sents:   26832; bsz: 4005/3525/67; 12831/11294 tok/s;    125 sec;\n",
            "[2023-04-21 20:40:20,734 INFO] Step 200/ 3000; acc: 31.5; ppl: 144.0; xent: 5.0; lr: 0.00056; sents:   24039; bsz: 4019/3525/60; 22020/19315 tok/s;    198 sec;\n",
            "[2023-04-21 20:41:33,255 INFO] Step 300/ 3000; acc: 41.9; ppl:  60.0; xent: 4.1; lr: 0.00084; sents:   25513; bsz: 4011/3520/64; 22126/19414 tok/s;    270 sec;\n",
            "[2023-04-21 20:42:45,130 INFO] Step 400/ 3000; acc: 50.4; ppl:  34.2; xent: 3.5; lr: 0.00112; sents:   25897; bsz: 4015/3528/65; 22343/19634 tok/s;    342 sec;\n",
            "[2023-04-21 20:43:57,460 INFO] Step 500/ 3000; acc: 60.2; ppl:  20.9; xent: 3.0; lr: 0.00140; sents:   26869; bsz: 4005/3528/67; 22150/19509 tok/s;    415 sec;\n",
            "[2023-04-21 20:45:09,462 INFO] Step 600/ 3000; acc: 66.2; ppl:  15.6; xent: 2.7; lr: 0.00168; sents:   26554; bsz: 4005/3523/66; 22248/19570 tok/s;    487 sec;\n",
            "[2023-04-21 20:46:20,682 INFO] Step 700/ 3000; acc: 70.1; ppl:  12.9; xent: 2.6; lr: 0.00196; sents:   29525; bsz: 3998/3535/74; 22455/19857 tok/s;    558 sec;\n",
            "\n",
            "\n",
            "\n",
            "yes\n",
            "[2023-04-21 20:47:32,899 INFO] Step 800/ 3000; acc: 72.9; ppl:  11.3; xent: 2.4; lr: 0.00224; sents:   24216; bsz: 4015/3526/61; 22241/19528 tok/s;    630 sec;\n",
            "[2023-04-21 20:48:44,487 INFO] Step 900/ 3000; acc: 74.8; ppl:  10.4; xent: 2.3; lr: 0.00252; sents:   25622; bsz: 4017/3528/64; 22444/19714 tok/s;    702 sec;\n",
            "[2023-04-21 20:48:49,429 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19644)\n",
            "\n",
            "[2023-04-21 20:48:49,430 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2023-04-21 20:48:53,762 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2023-04-21 20:49:03,849 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2023-04-21 20:49:13,202 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2023-04-21 20:49:19,970 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2023-04-21 20:51:01,369 INFO] Step 1000/ 3000; acc: 76.4; ppl:   9.5; xent: 2.3; lr: 0.00279; sents:   25627; bsz: 4012/3530/64; 11724/10315 tok/s;    838 sec;\n",
            "[2023-04-21 20:51:05,984 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 4.611226558685303 s.\n",
            "[2023-04-21 20:51:05,986 INFO] Train perplexity: 34.1283\n",
            "[2023-04-21 20:51:05,987 INFO] Train accuracy: 55.392\n",
            "[2023-04-21 20:51:05,987 INFO] Sentences processed: 260694\n",
            "[2023-04-21 20:51:05,987 INFO] Average bsz: 4010/3527/65\n",
            "[2023-04-21 20:51:05,987 INFO] Validation perplexity: 10.447\n",
            "[2023-04-21 20:51:05,987 INFO] Validation accuracy: 76.0618\n",
            "[2023-04-21 20:51:05,987 INFO] Model is improving ppl: inf --> 10.447.\n",
            "[2023-04-21 20:51:05,987 INFO] Model is improving acc: -inf --> 76.0618.\n",
            "[2023-04-21 20:51:06,001 INFO] Saving checkpoint models/model.fren_step_1000.pt\n",
            "[2023-04-21 20:52:23,172 INFO] Step 1100/ 3000; acc: 77.8; ppl:   8.9; xent: 2.2; lr: 0.00266; sents:   24420; bsz: 4015/3519/61; 19635/17209 tok/s;    920 sec;\n",
            "[2023-04-21 20:53:35,365 INFO] Step 1200/ 3000; acc: 79.5; ppl:   8.4; xent: 2.1; lr: 0.00255; sents:   28221; bsz: 4002/3530/71; 22175/19556 tok/s;    992 sec;\n",
            "[2023-04-21 20:54:47,535 INFO] Step 1300/ 3000; acc: 82.3; ppl:   7.4; xent: 2.0; lr: 0.00245; sents:   26467; bsz: 4008/3525/66; 22215/19535 tok/s;   1065 sec;\n",
            "[2023-04-21 20:56:00,240 INFO] Step 1400/ 3000; acc: 83.9; ppl:   6.9; xent: 1.9; lr: 0.00236; sents:   25089; bsz: 4016/3528/63; 22097/19411 tok/s;   1137 sec;\n",
            "[2023-04-21 20:57:12,808 INFO] Step 1500/ 3000; acc: 85.7; ppl:   6.4; xent: 1.9; lr: 0.00228; sents:   24862; bsz: 4013/3522/62; 22123/19414 tok/s;   1210 sec;\n",
            "[2023-04-21 20:58:25,052 INFO] Step 1600/ 3000; acc: 84.1; ppl:   6.8; xent: 1.9; lr: 0.00221; sents:   27177; bsz: 4007/3529/68; 22189/19537 tok/s;   1282 sec;\n",
            "[2023-04-21 20:59:37,279 INFO] Step 1700/ 3000; acc: 86.9; ppl:   6.1; xent: 1.8; lr: 0.00214; sents:   26370; bsz: 4011/3525/66; 22215/19520 tok/s;   1354 sec;\n",
            "[2023-04-21 21:00:49,418 INFO] Step 1800/ 3000; acc: 87.5; ppl:   5.9; xent: 1.8; lr: 0.00208; sents:   26340; bsz: 4006/3525/66; 22211/19548 tok/s;   1427 sec;\n",
            "[2023-04-21 21:00:58,442 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19693)\n",
            "\n",
            "[2023-04-21 21:00:58,442 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2023-04-21 21:01:04,582 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2023-04-21 21:01:13,591 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2023-04-21 21:01:19,379 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2023-04-21 21:01:30,168 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2023-04-21 21:03:01,247 INFO] Step 1900/ 3000; acc: 89.9; ppl:   5.4; xent: 1.7; lr: 0.00203; sents:   27411; bsz: 4008/3535/69; 12160/10727 tok/s;   1558 sec;\n",
            "[2023-04-21 21:04:13,122 INFO] Step 2000/ 3000; acc: 90.4; ppl:   5.3; xent: 1.7; lr: 0.00198; sents:   27316; bsz: 4006/3527/68; 22296/19631 tok/s;   1630 sec;\n",
            "[2023-04-21 21:04:13,369 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=157)\n",
            "\n",
            "[2023-04-21 21:04:17,333 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 4.208382606506348 s.\n",
            "[2023-04-21 21:04:17,335 INFO] Train perplexity: 15.0996\n",
            "[2023-04-21 21:04:17,335 INFO] Train accuracy: 70.0903\n",
            "[2023-04-21 21:04:17,335 INFO] Sentences processed: 524367\n",
            "[2023-04-21 21:04:17,335 INFO] Average bsz: 4010/3527/66\n",
            "[2023-04-21 21:04:17,336 INFO] Validation perplexity: 6.81307\n",
            "[2023-04-21 21:04:17,336 INFO] Validation accuracy: 86.5402\n",
            "[2023-04-21 21:04:17,336 INFO] Model is improving ppl: 10.447 --> 6.81307.\n",
            "[2023-04-21 21:04:17,336 INFO] Model is improving acc: 76.0618 --> 86.5402.\n",
            "[2023-04-21 21:04:17,354 INFO] Saving checkpoint models/model.fren_step_2000.pt\n",
            "[2023-04-21 21:05:33,645 INFO] Step 2100/ 3000; acc: 90.9; ppl:   5.2; xent: 1.6; lr: 0.00193; sents:   25108; bsz: 4015/3524/63; 19943/17507 tok/s;   1711 sec;\n",
            "[2023-04-21 21:06:46,353 INFO] Step 2200/ 3000; acc: 91.4; ppl:   5.1; xent: 1.6; lr: 0.00188; sents:   25280; bsz: 4011/3525/63; 22068/19392 tok/s;   1783 sec;\n",
            "[2023-04-21 21:07:58,727 INFO] Step 2300/ 3000; acc: 92.0; ppl:   5.0; xent: 1.6; lr: 0.00184; sents:   25786; bsz: 4012/3524/64; 22173/19475 tok/s;   1856 sec;\n",
            "[2023-04-21 21:09:11,212 INFO] Step 2400/ 3000; acc: 92.2; ppl:   4.9; xent: 1.6; lr: 0.00180; sents:   25225; bsz: 4004/3512/63; 22098/19381 tok/s;   1928 sec;\n",
            "[2023-04-21 21:10:23,695 INFO] Step 2500/ 3000; acc: 92.6; ppl:   4.9; xent: 1.6; lr: 0.00177; sents:   25642; bsz: 4012/3526/64; 22142/19461 tok/s;   2001 sec;\n",
            "[2023-04-21 21:11:35,666 INFO] Step 2600/ 3000; acc: 93.0; ppl:   4.8; xent: 1.6; lr: 0.00173; sents:   27558; bsz: 4007/3532/69; 22271/19630 tok/s;   2073 sec;\n",
            "[2023-04-21 21:12:48,268 INFO] Step 2700/ 3000; acc: 93.3; ppl:   4.7; xent: 1.6; lr: 0.00170; sents:   26099; bsz: 4011/3524/65; 22098/19418 tok/s;   2145 sec;\n",
            "[2023-04-21 21:13:06,120 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=19688)\n",
            "\n",
            "[2023-04-21 21:13:06,120 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2023-04-21 21:13:15,832 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2023-04-21 21:13:20,065 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2023-04-21 21:13:30,688 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2023-04-21 21:15:08,890 INFO] Step 2800/ 3000; acc: 94.2; ppl:   4.6; xent: 1.5; lr: 0.00167; sents:   26117; bsz: 4012/3530/65; 11412/10041 tok/s;   2286 sec;\n",
            "[2023-04-21 21:16:21,590 INFO] Step 2900/ 3000; acc: 94.3; ppl:   4.6; xent: 1.5; lr: 0.00164; sents:   25544; bsz: 4012/3524/64; 22078/19390 tok/s;   2359 sec;\n",
            "[2023-04-21 21:17:33,904 INFO] Step 3000/ 3000; acc: 94.4; ppl:   4.5; xent: 1.5; lr: 0.00161; sents:   25259; bsz: 4016/3528/63; 22217/19513 tok/s;   2431 sec;\n",
            "[2023-04-21 21:17:34,230 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=157)\n",
            "\n",
            "[2023-04-21 21:17:38,455 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 4.5471813678741455 s.\n",
            "[2023-04-21 21:17:38,458 INFO] Train perplexity: 10.3263\n",
            "[2023-04-21 21:17:38,458 INFO] Train accuracy: 77.6629\n",
            "[2023-04-21 21:17:38,458 INFO] Sentences processed: 781985\n",
            "[2023-04-21 21:17:38,458 INFO] Average bsz: 4010/3526/65\n",
            "[2023-04-21 21:17:38,458 INFO] Validation perplexity: 6.35502\n",
            "[2023-04-21 21:17:38,458 INFO] Validation accuracy: 88.4575\n",
            "[2023-04-21 21:17:38,458 INFO] Model is improving ppl: 6.81307 --> 6.35502.\n",
            "[2023-04-21 21:17:38,458 INFO] Model is improving acc: 86.5402 --> 88.4575.\n",
            "[2023-04-21 21:17:38,477 INFO] Saving checkpoint models/model.fren_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model and Results\n",
        "\n",
        "Translation Options:\n",
        "* `-model` - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n",
        "* `-src` - the subworded test dataset, source file\n",
        "* `-output` - give any file name to the new translation output file\n",
        "* `-gpu` - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n",
        "* `-min_length` - [optional] to avoid empty translations\n",
        "* `-verbose` - [optional] if you want to print translations\n",
        "\n",
        "Refer to [OpenNMT-py translation options](https://opennmt.net/OpenNMT-py/options/translate.html) for more details."
      ],
      "metadata": {
        "id": "QjLFU15xr8z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate the \"subworded\" source file of the test dataset\n",
        "# Change the model name, if needed.\n",
        "!onmt_translate -model models/model.fren_step_3000.pt -src UN.en-fr.fr-filtered.fr.subword.test -output UN.en.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Y6v3nUiMgd",
        "outputId": "192e565b-1abb-4199-98eb-a6adc88e8fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-04-21 21:22:48,944 INFO] PRED SCORE: -0.1999, PRED PPL: 1.22 NB SENTENCES: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first 5 lines of the translation file\n",
        "!head -n 5 UN.en.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3daJ0JwJsCa2",
        "outputId": "d4956083-7c4b-4fd1-f2e6-43f1d2847702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁ • ▁Exchang e ▁of ▁programmes ▁aimed ▁at ▁developing ▁a ▁dialogue ▁and ▁understanding ▁of ▁the ▁scourge s ▁of ▁racism , ▁violence ▁and ▁racism ▁among ▁peoples , ▁in ▁particular ▁among ▁young ▁people ;\n",
            "▁Guid ed ▁by ▁international ▁legal ▁instruments , ▁documents ▁and ▁recommendations ▁against ▁corruption\n",
            "▁ 2 7 . ▁Calls ▁upon ▁States ▁and ▁private ▁entities ▁concerned ▁to ▁cooperate ▁fully ▁with ▁the ▁International ▁ Maritime ▁Organization , ▁including ▁by ▁submitt ing ▁reports ▁on ▁incidents ▁to ▁the ▁organization ▁and ▁by ▁implementing ▁its ▁guidelines ▁on ▁preventing ▁attacks ▁of ▁piracy ▁and ▁ armed ▁robbery ;\n",
            "▁Recalling ▁the ▁Unit ed ▁Nations ▁Millennium ▁Declaration See ▁resolution ▁ 5 5 / 2 . ▁and ▁the ▁outcome ▁documents ▁of ▁the ▁twenty - third Resolution ▁S - 2 3 / 2 , ▁annex , ▁and ▁S - 2 3 / 3 , ▁annex . ▁and ▁twenty - fourth Resolution ▁S - 2 4 / 2 , ▁annex . ▁special ▁sessions ▁of ▁the ▁General ▁Assembly , ▁held , ▁respectively , ▁in ▁New ▁York ▁from ▁ 5 ▁to ▁ 1 0 ▁June ▁ 2 0 0 0 ▁and ▁in ▁Geneva ▁from ▁ 2 6 ▁June ▁to ▁ 1 ▁July ▁ 2 0 0 0 ,\n",
            "▁ 1 3 . ▁Requests ▁the ▁Secretary - General ▁to ▁continue ▁to ▁monitor ▁the ▁implementation ▁of ▁resolution ▁ 4 1 / 1 1 ▁and ▁subsequent ▁resolutions ▁on ▁the ▁matter , ▁and ▁to ▁report ▁to ▁the ▁General ▁Assembly ▁at ▁its ▁fifty - eighth ▁session , ▁taking ▁into ▁account , ▁inter ▁alia , ▁the ▁views ▁expressed ▁by ▁member ▁States ;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed install/update sentencepiece\n",
        "!pip3 install --upgrade -q sentencepiece\n",
        "\n",
        "# Desubword the translation file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do-yK66hscKV",
        "outputId": "2bf97645-194f-4f85-cbd8-c0c6412c180b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: UN.en.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first 5 lines of the desubworded translation file\n",
        "!head -n 5 UN.en.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPGk-6a5sdo8",
        "outputId": "75c147a2-23ba-4e9d-b0f9-95f4d5717a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• Exchange of programmes aimed at developing a dialogue and understanding of the scourges of racism, violence and racism among peoples, in particular among young people;\n",
            "Guided by international legal instruments, documents and recommendations against corruption\n",
            "27. Calls upon States and private entities concerned to cooperate fully with the International Maritime Organization, including by submitting reports on incidents to the organization and by implementing its guidelines on preventing attacks of piracy and armed robbery;\n",
            "Recalling the United Nations Millennium DeclarationSee resolution 55/2. and the outcome documents of the twenty-thirdResolution S-23/2, annex, and S-23/3, annex. and twenty-fourthResolution S-24/2, annex. special sessions of the General Assembly, held, respectively, in New York from 5 to 10 June 2000 and in Geneva from 26 June to 1 July 2000,\n",
            "13. Requests the Secretary-General to continue to monitor the implementation of resolution 41/11 and subsequent resolutions on the matter, and to report to the General Assembly at its fifty-eighth session, taking into account, inter alia, the views expressed by member States;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Desubword the target file (reference) of the test dataset\n",
        "# Note: You might as well have split files *before* subwording during dataset preperation, \n",
        "# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model UN.en-fr.en-filtered.en.subword.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGfBq0F0sfLM",
        "outputId": "da2137b1-e4aa-4f95-d32a-dac39e5b6aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: UN.en-fr.en-filtered.en.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first 5 lines of the desubworded reference\n",
        "!head -n 5 UN.en-fr.en-filtered.en.subword.test.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQviYhqBsgJM",
        "outputId": "a3d43da8-c7f6-4bf4-c9fb-f3c8f92ca1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• Implementation of programmes to enhance the spirit of dialogue, understanding and rejection of intolerance, violence and racism among people, particularly the youth;\n",
            "Indicative list of international legal instruments, documents and recommendations against corruption\n",
            "27. Calls upon States and private entities concerned to cooperate fully with the International Maritime Organization, including by submitting reports on incidents to the organization and by implementing its guidelines on preventing attacks of piracy and armed robbery;\n",
            "Recalling the United Nations Millennium DeclarationSee resolution 55/2. and the outcome documents of the twenty-thirdResolutions S-23/2, annex, and S-23/3, annex. and twenty-fourthResolution S-24/2, annex. special sessions of the General Assembly, held, respectively, in New York from 5 to 10 June 2000 and Geneva from 26 June to 1 July 2000,\n",
            "13. Requests the Secretary-General to keep the implementation of resolution 41/11 and subsequent resolutions on the matter under review and to submit a report to the General Assembly at its fifty-eighth session, taking into account, inter alia, the views expressed by Member States;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "We are using BLEU for evaluation. Files must be detokenized/desubworded beforehand."
      ],
      "metadata": {
        "id": "PexKBYJAshdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the BLEU script\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLFBpXHvsjb0",
        "outputId": "18e55330-0f6c-451c-99aa-f6bcdb29e6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-21 21:24:16--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957 [text/plain]\n",
            "Saving to: ‘compute-bleu.py’\n",
            "\n",
            "\rcompute-bleu.py       0%[                    ]       0  --.-KB/s               \rcompute-bleu.py     100%[===================>]     957  --.-KB/s    in 0s      \n",
            "\n",
            "2023-04-21 21:24:16 (33.5 MB/s) - ‘compute-bleu.py’ saved [957/957]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sacrebleu\n",
        "!pip3 install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Ma75VuskgU",
        "outputId": "4cc94688-cfe6-4cea-b9e2-2785288d8d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.9/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the translation (without subwording)\n",
        "!python3 compute-bleu.py UN.en-fr.en-filtered.en.subword.test.desubword UN.en.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuxRMM81sl50",
        "outputId": "d938a0fc-9fdc-43dd-fb6d-8a607d96dcce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: • Implementation of programmes to enhance the spirit of dialogue, understanding and rejection of intolerance, violence and racism among people, particularly the youth;\n",
            "MTed 1st sentence: • Exchange of programmes aimed at developing a dialogue and understanding of the scourges of racism, violence and racism among peoples, in particular among young people;\n",
            "BLEU:  68.40297343405935\n"
          ]
        }
      ]
    }
  ]
}